{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "polar-advocacy",
   "metadata": {},
   "source": [
    "# Is Monday Effect an Urban Myth?\n",
    "\n",
    "**Authors**: Blythe King, Dinggyue Lie, Lucy (Yu) Xue\\*, Sungbin Youk\\\n",
    "**Date**: May 30th, 2021\\\n",
    "**Description**: As a final project for PSTAT 234 in University of California, Santa Barbara, the authors examined the presence of Monday effect. \\\n",
    "The authors equally contributed to the project. The names of the authors are in an alphabetical order. The corresponding author is indicated with *.\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-nature",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "1. [Introduction](#introduction)\\\n",
    "    A. [Predicting the Stock Market](#predicting-the-stock-market)\\\n",
    "    B. [What is Monday Effect](#what-is-monday-effect)\\\n",
    "    C. [Our Objectives](#our-objectives)\n",
    "2. [Tackling Objective 1](#tackling-objective-1)\n",
    "3. [Tackling Objective 2](#tackling-objective-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-wilson",
   "metadata": {},
   "source": [
    "## Introduction<a clas =\"anchor\" id = \"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-great",
   "metadata": {},
   "source": [
    "### Predicting the Stock Market <a clas =\"anchor\" id = \"predicting-the-stock-market\"></a>\n",
    "\n",
    "It will be great if you can predict the changes in the stock market. It will make you rich. Isn't that everyone's dream? Unfortunately, [efficient market hypothesis](https://www.investopedia.com/terms/e/efficientmarkethypothesis.asp) postulates that generating a stable parameter that reflects the share prices is impossible as the share prices reflect all information. \n",
    "\n",
    "It would be against the efficient market hypothesis if there is a predictable *pattern* in the stock market. In 1973, [Frank Cross](https://www.jstor.org/stable/pdf/4529641.pdf?refreqid=excelsior%3Adeff8e6e9e2c4c0b275b4b03a21b9c13) documented a non-random movement in stock prices. Here are the main findings from examining the Standard & Poor's Composite Stock Index from 1953 to 1970:\n",
    "- The index have risen on Friday more often than on any other days of the week, and have risen least often on Monday. \n",
    "- When the Friday index declined, the Monday index was more likely to also see a decline. When the Friday index advanced, the Monday index was likely to remain static (neither advancing nor declining). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-matrix",
   "metadata": {},
   "source": [
    "### What is Monday Effect? <a clas =\"anchor\" id = \"what-is-monday-effect\"></a>\n",
    "\n",
    "Over the years, Frank Cross's findings were coined into what is now known as the **Monday Effect**. There are two different definitions of the monday effect (each corresponding to the two findings that are mentioned above). \n",
    "\n",
    "- Monday effect states that the returns on Monday are less than the other days of the week, and are often negative on average ([Pettengill, 2003](https://www.jstor.org/stable/pdf/23292837.pdf?refreqid=excelsior%3A6da162ff7d91746d901fc154171e6015)).\n",
    "- Monday effect states that the returns on the stock market on Monday, especially the first few hours, will follow the pattern of the previous Friday, espeically the last few hours ([Investopedia](https://www.investopedia.com/terms/m/mondayeffect.asp)). \n",
    "\n",
    "You may wonder what may be the reason behind this abnormality in the stock prices. As the existence of Monday effect is controversial (thus, the reason for our project), there isn't a clear answer. Some state that the stock returns are low on Monday because companies may hold on to bad news until the last day of stock trading (Friday), which in turn makes the next stock trading day (Monday) to take the hit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-conversion",
   "metadata": {},
   "source": [
    "### Our Objectives <a clas =\"anchor\" id = \"our-objectives\"></a>\n",
    "\n",
    "The objective of our project is in three-folds:\n",
    "1) The first objective is to examine the first definitoin of Monday effect (i.e. the stock returns of Monday is less than the other days of the week). We will benchmark [Arman and Lestari's study](https://www.atlantis-press.com/proceedings/icame-18/125917114). They examined Monday effect in the Indonesian Stock Exchange. We will examine if Monday effect is also present in the U.S. stock market.\\\n",
    "2) The second objective of this study is to examine the second definition of Monday effect. We will examine if Monday's returns are correlated to that of Friday.\\\n",
    "3) We will take a step further and apply time series analysis to see \"abnormailities\" is stock returns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-pledge",
   "metadata": {},
   "source": [
    "----\n",
    "## Tackling Objective 1 <a clas =\"anchor\" id = \"tackling-objective-1\"></a>\n",
    "\n",
    "In our analysis, the stock return data are obtained from yfinance package in python. We will create two sets of data. The first data will include the stock returns of S&P 500 from 2014 to 2017. This data set has the identical timeframe to that of Arman and Lestari's study. The second dataset includes the latest S&P 500 constituents. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-bargain",
   "metadata": {},
   "source": [
    "### A Short Summary of Arman and Lestari's Study\n",
    "\n",
    "Arman and Lestari examined the Monday effect by examining the banking sectors on the Indonesian stock market from 2014 to 2017. A one-sample t-test was conducted for each of the weekdays. The results indicated that the average stock return on Monday is -0.0006, which was not statistically significant. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-monster",
   "metadata": {},
   "source": [
    "### Importing Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-shame",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import datetime\n",
    "from datetime import date\n",
    "import calendar\n",
    "import io\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-working",
   "metadata": {},
   "source": [
    "### Importing the list of ticker for S&P 500 between 2014 to 2017\n",
    "\n",
    "The first step is to retrieve the companies that constituted S&P 500 in the past. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-sharing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the csv file from a Github page which has a list of companies and when they were added or removed from S&P 500\n",
    "url = \"https://raw.githubusercontent.com/leosmigel/analyzingalpha/master/sp500-historical-components-and-changes/sp500_history.csv\"\n",
    "download = requests.get(url).content\n",
    "\n",
    "# Reading the downloaded content and turning it into a pandas dataframe\n",
    "df = pd.read_csv(io.StringIO(download.decode('utf-8')))\n",
    "\n",
    "#Turning the date column into a datetime object\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "# Printing out the first 5 rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-indonesia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve the tickers in S&P 500 for a given timeframe\n",
    "def past_SP_ticker(end_date):\n",
    "    ticker_list = []\n",
    "    global df\n",
    "    for index,row in df.iterrows():\n",
    "        if row['date'] > end_date:\n",
    "            break\n",
    "        else:\n",
    "            if row['variable'] == \"added_ticker\":\n",
    "                ticker_list.append(row['value'])\n",
    "            elif row['value'] in ticker_list:\n",
    "                ticker_list.remove(row['value'])\n",
    "    return ticker_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the past_SP_ticker() function to retrieve the tickers of S&P 500 for 2017. \n",
    "end_date = '20171231'\n",
    "date_time_obj = datetime.datetime.strptime(end_date,'%Y%m%d')\n",
    "SP_ticker_2017 = past_SP_ticker(date_time_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-commissioner",
   "metadata": {},
   "source": [
    "### Creating a dataframe of stock returns for the identified S&P 500 constituents of 2017\n",
    "The next step is to obtain the daily stock returns of the selected companies. This requires several steps: obtain the stock data of the S&P 500 constituents of 2017, delete the missing values, calculate the log retruns, create a multilevel index (i.e., hierarchical index) with the days of the week"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-performance",
   "metadata": {},
   "source": [
    "#### Obtaining the stock data of S&P 500 constituents of 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-sigma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the ticker to obtain stock prices from yfinance\n",
    "rawdata = yf.download(SP_ticker_2017, start=\"2013-12-31\", end=\"2017-12-31\")\n",
    "rawdata.columns = rawdata.columns.set_names(['Value', 'Symbol'])\n",
    "rawdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-turkish",
   "metadata": {},
   "source": [
    "#### Deleting the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the missing values in terms of rows\n",
    "rawdata['Close'].isna().sum(axis=0).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-livestock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making an list of tuples for tickers that has more than 3 missing values\n",
    "high_missing_ticker = rawdata['Close'].isna().sum(axis=0) > 3\n",
    "high_missing_ticker_list = high_missing_ticker[high_missing_ticker].index.tolist()\n",
    "high_missing_ticker_tuples = list()\n",
    "for i in ['Adj Close', 'Open', 'Close', 'High' ,'Low', 'Volume']:\n",
    "    high_missing_ticker_tuples += list(zip([i]*len(high_missing_ticker_list),high_missing_ticker_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding columns (i.e., tickers) that has more than 3 missing values \n",
    "rawdata = rawdata.drop(high_missing_ticker_tuples, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-november",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding out the dates that all tickers (columns) have missing values\n",
    "missingdate =rawdata.isna().sum(axis=1) > 0\n",
    "missingdate[missingdate].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f15def1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the first row if it has missing values\n",
    "if rawdata.iloc[0].isna().sum().any():\n",
    "    rawdata = rawdata.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-lingerie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other rows with missing values are replaced with the values from the previous date\n",
    "rawdata = rawdata.fillna(method= 'ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-mexico",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Double check to see if all the missing values were either removed or replaced\n",
    "(rawdata.isna().sum(axis=None)>0).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-hearing",
   "metadata": {},
   "source": [
    "#### Calculating the log returns for closing price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the log returns from stock prices\n",
    "logret = np.log(rawdata['Close']).diff()\n",
    "logret.columns = pd.MultiIndex.from_product([['logreturn'], logret.columns])\n",
    "# Joining logret and rawdata \n",
    "rawdata = rawdata.join(logret)\n",
    "# row with the index of 2013-12-31 will be deleted as it is out of the scope of our data (2014~2017)\n",
    "rawdata = rawdata.drop(pd.Timestamp('2013-12-31'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After preprocessing the data, we have idenified the log returns of {} companies, which were included in S&P500 in 2017. To recap, we are examining the stock returns from 2014 to 2017. Therefore, we will be examining the stock returns of {} days\".format(len(logret.columns), len(logret)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-champion",
   "metadata": {},
   "source": [
    "#### Creating a new columns for the industry information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-darwin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the information about the tickers that are included in SP_ticker_2017\n",
    "industry_dic = dict()\n",
    "for item in SP_ticker_2017:\n",
    "    try:\n",
    "        industry_dic[item] = yf.Ticker(item).info['industry']\n",
    "    except:\n",
    "        industry_dic[item] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the industry to a level of the column\n",
    "rawdata.columns = pd.MultiIndex.from_tuples([(value, industry_dic[ticker], ticker) for value, ticker in rawdata.columns])\n",
    "rawdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-toner",
   "metadata": {},
   "source": [
    "#### Creating a new columns for days of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-following",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The day of the week is added as a new index (creating a hierarchical index)\n",
    "rawdata['days'] = [calendar.day_name[day.weekday()] for day in rawdata.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-macintosh",
   "metadata": {},
   "source": [
    "#### Exporting dataframe as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-husband",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata.to_csv('SP500_2014_2017_multilevel.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-auckland",
   "metadata": {},
   "source": [
    "#### Staking the Closing and Log returns into one dataframe\n",
    "\n",
    "For convenience in running some of the statistical analyses, the multilevel data of stock values are stacked into a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-copyright",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To easily stack the data, it is easier to delete the days and industry information.\n",
    "rawdata = rawdata.drop('days', axis =1)\n",
    "rawdata.columns = rawdata.columns.droplevel(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is stacked \n",
    "stacked_rawdata = rawdata.stack()\n",
    "stacked_rawdata.reset_index(inplace=True)\n",
    "stacked_rawdata = stacked_rawdata.rename(columns = {'level_1':'Ticker'})\n",
    "\n",
    "# Days are added as a new column\n",
    "stacked_rawdata['days'] = [calendar.day_name[day.weekday()] for day in stacked_rawdata['Date']]\n",
    "\n",
    "# Industry information is added as a new column\n",
    "stacked_rawdata['Industry'] = [industry_dic[ticker] for ticker in stacked_rawdata['Ticker']]\n",
    "stacked_rawdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-convertible",
   "metadata": {},
   "source": [
    "#### Exporting dataframe as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-personal",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_rawdata.to_csv('SP500_2014_2017_stacked.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c750fce",
   "metadata": {},
   "source": [
    "Let's repeat what we just did to create the stock returns data for the latest S&P 500 constituents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-fluid",
   "metadata": {},
   "source": [
    "### Creating a dataframe of stock returns for the latest S&P 500 constituents\n",
    "The list of constituents for the latest S&P500 are available in `list of sp500.xlsx` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-baker",
   "metadata": {},
   "outputs": [],
   "source": [
    "SP500list = pd.read_excel('list of sp500.xlsx', engine='openpyxl')\n",
    "SP_ticker_2020 = SP500list['Symbol'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-variation",
   "metadata": {},
   "source": [
    "### Obtaining the stock data of latest S&P 500 constituents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the ticker to obtain stock prices from yfinance\n",
    "rawdata = yf.download(SP_ticker_2020, start=\"2017-12-30\", end=\"2021-05-28\")\n",
    "rawdata.columns = rawdata.columns.set_names(['Value', 'Symbol'])\n",
    "rawdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-brief",
   "metadata": {},
   "source": [
    "#### Deleting the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the missing values in terms of rows\n",
    "rawdata['Close'].isna().sum(axis=0).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-cabinet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making an list of tuples for tickers that has more than 3 missing values\n",
    "high_missing_ticker = rawdata['Close'].isna().sum(axis=0) > 3\n",
    "high_missing_ticker_list = high_missing_ticker[high_missing_ticker].index.tolist()\n",
    "high_missing_ticker_tuples = list()\n",
    "for i in ['Adj Close', 'Open', 'Close', 'High' ,'Low', 'Volume']:\n",
    "    high_missing_ticker_tuples += list(zip([i]*len(high_missing_ticker_list),high_missing_ticker_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-order",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding columns (i.e., tickers) that has more than 3 missing values \n",
    "rawdata = rawdata.drop(high_missing_ticker_tuples, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-happening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding out the dates that all tickers (columns) have missing values\n",
    "missingdate =rawdata.isna().sum(axis=1) > 0\n",
    "missingdate[missingdate].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560d8bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the first row if it has missing values\n",
    "if rawdata.iloc[0].isna().sum().any():\n",
    "    rawdata = rawdata.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-segment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other rows with missing values are replaced with the values from the previous date\n",
    "rawdata = rawdata.fillna(method= 'ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check to see if all the missing values were either removed or replaced\n",
    "(rawdata.isna().sum(axis=None)>0).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-sunrise",
   "metadata": {},
   "source": [
    "#### Calculating the log returns for closing price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-congress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the log returns from stock prices\n",
    "logret = np.log(rawdata['Close']).diff()\n",
    "logret.columns = pd.MultiIndex.from_product([['logreturn'], logret.columns])\n",
    "# Joining logret and rawdata \n",
    "rawdata = rawdata.join(logret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After preprocessing the data, we have idenified the log returns of {} companies, which were included in S&P500 in 2021. To recap, we are examining the stock returns from 2018 to May 2021. Therefore, we will be examining the stock returns of {} days\".format(len(logret.columns), len(logret)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-casting",
   "metadata": {},
   "source": [
    "#### Creating a new columns for the industry information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-editor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the information about the tickers that are included in SP_ticker_2017\n",
    "industry_dic = dict()\n",
    "for item in SP_ticker_2020:\n",
    "    try:\n",
    "        industry_dic[item] = yf.Ticker(item).info['industry']\n",
    "    except:\n",
    "        industry_dic[item] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-astrology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the industry to a level of the column\n",
    "rawdata.columns = pd.MultiIndex.from_tuples([(value, industry_dic[ticker], ticker) for value, ticker in rawdata.columns])\n",
    "rawdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-skill",
   "metadata": {},
   "source": [
    "#### Creating a new columns for days of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The day of the week is added as a new index (creating a hierarchical index)\n",
    "rawdata['days'] = [calendar.day_name[day.weekday()] for day in rawdata.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-graphics",
   "metadata": {},
   "source": [
    "#### Exporting dataframe as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-weapon",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata.to_csv('SP500_2018_2021_multilevel.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-mercury",
   "metadata": {},
   "source": [
    "#### Staking the Closing and Log returns into one dataframe\n",
    "\n",
    "For convenience in running some of the statistical analyses, the multilevel data of stock values are stacked into a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-utilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To easily stack the data, it is easier to delete the days and industry information.\n",
    "rawdata = rawdata.drop('days', axis =1)\n",
    "rawdata.columns = rawdata.columns.droplevel(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is stacked \n",
    "stacked_rawdata = rawdata.stack()\n",
    "stacked_rawdata.reset_index(inplace=True)\n",
    "stacked_rawdata = stacked_rawdata.rename(columns = {'level_1':'Ticker'})\n",
    "\n",
    "# Days are added as a new column\n",
    "stacked_rawdata['days'] = [calendar.day_name[day.weekday()] for day in stacked_rawdata['Date']]\n",
    "\n",
    "# Industry information is added as a new column\n",
    "stacked_rawdata['Industry'] = [industry_dic[ticker] for ticker in stacked_rawdata['Ticker']]\n",
    "stacked_rawdata.head()\n",
    "\n",
    "# Before we forget, let's add the days and industry column back to our rawdata. We will comeback to this later\n",
    "rawdata.columns = pd.MultiIndex.from_tuples([(value, industry_dic[ticker], ticker) for value, ticker in rawdata.columns])\n",
    "rawdata['days'] = [calendar.day_name[day.weekday()] for day in rawdata.index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-delight",
   "metadata": {},
   "source": [
    "#### Exporting dataframe as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_rawdata.to_csv('SP500_2018_2021_stacked.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-hammer",
   "metadata": {},
   "source": [
    "### Analyzing the Monday Effect\n",
    "The next step is to analyze the Monday effect. First, as done in Arman and Lestari's research, one-sample t-test is conducted for each day of the week. The test value is 0. Therefore, a significant result indicates that it is highly unlikely to have obtained the average log stock returns on a specific day of the week given that the null hypothesis is true (i.e. the average log stock return is 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-script",
   "metadata": {},
   "source": [
    "##### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-serbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine how many columns we have\n",
    "print('There are {} columns in our dataset'.format(len(rawdata.columns)))\n",
    "# The days of the week is one of the columns\n",
    "print('Is days one of the columns?','days' in rawdata.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-triple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we move on, let's make a multilevel index for the rows.\n",
    "rawdata.set_index('days', append=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency count for each day\n",
    "rawdata['logreturn'].groupby(level=1).count().mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we get into conducting one sample t-test, \n",
    "# let's look at the mean of log stock returns for each day of the week\n",
    "rawdata['logreturn'].groupby(level=1).mean().mean(axis=1).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the standard deviation of log stock returns for each day of the week\n",
    "rawdata['logreturn'].groupby(level=1).std().std(axis=1).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-blanket",
   "metadata": {},
   "source": [
    "Although less tha Thursday, Monday has a negative log stock returns and a largest variance. Let's see if this value is statistically significant. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-glory",
   "metadata": {},
   "source": [
    "##### Inferential Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-sussex",
   "metadata": {},
   "source": [
    "###### t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_sample_t(day):\n",
    "    a = rawdata.xs(day, level='days').logreturn.values.flatten()\n",
    "    a = a[~numpy.isnan(a)]\n",
    "    return stats.ttest_1samp(a,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-increase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-sample t-test for Monday\n",
    "one_sample_t('Monday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-shower",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-sample t-test for Tuesday\n",
    "one_sample_t('Tuesday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-mortgage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-sample t-test for Wednesday\n",
    "one_sample_t('Wednesday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-creature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-sample t-test for Thursday\n",
    "one_sample_t('Thursday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-sample t-test for Friday\n",
    "one_sample_t('Friday')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-louisiana",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-buddy",
   "metadata": {},
   "source": [
    "## Tackling Objective 2 <a clas =\"anchor\" id = \"tackling-objective-2\"></a>\n",
    "\n",
    "In this section, we will examine Monday effect in term of its second defintion. We will use the same data that were used in the previous section. \n",
    "\n",
    "Disclaimer: did you know that you can run R codes in a python based jupyter notebook? Well, the environment that this notebook is built on allows for that with the use of [rpy2](https://pypi.org/project/rpy2/) package. We simply use `%%R` in the cell that we want to run R codes. `%%R` is called R magic (and yes, magic indeed). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06605ec",
   "metadata": {},
   "source": [
    "### Setting up R and Importing R libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's import rpy2\n",
    "import rpy2\n",
    "#The following code enables you to use R magic\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-graphic",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "#Let's load the R packages that we will use, we will use `pacman` library to help us load all other libraries \n",
    "install.packages(\"pacman\")\n",
    "library(pacman)\n",
    "p_load(dplyr, plyr, readr, tidyr, reshape, ggplot2, sjPlot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c12ad4",
   "metadata": {},
   "source": [
    "### Importing the data for S&P 500 constituents in 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672d6d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "#Let's read in the .csv file of the stock returns for S&P 500 constituents in 2017\n",
    "SP500_2014_2017_stacked <- read_csv(\"SP500_2014_2017_stacked.csv\")\n",
    "\n",
    "#We will sort the dataset by company (i.e. tickers)\n",
    "all_data <- SP500_2014_2017_stacked[order(SP500_2014_2017_stacked$Ticker),]\n",
    "\n",
    "#The numeric date will be used as an index to pair log returns of two consecutive days\n",
    "Date_num = as.numeric(all_data$Date)\n",
    "all_data[\"Date_num\"]=Date_num \n",
    "\n",
    "#Before we move on, let's clean the data by removing the date variable\n",
    "all_data_no_date = subset(all_data, select = -Date)\n",
    "\n",
    "# Let's generate a list of unique companies (i.e. unique tickers)\n",
    "Ticker_list = unique(all_data_no_date$Ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087e812c",
   "metadata": {},
   "source": [
    "### Seperating the dataset based on the days of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812f16ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "#We wil now make pairs of og-return for two consecutive days\n",
    "Monday = Tuesday = Wednesday = Thursday = Friday = Next_Monday = {}\n",
    "\n",
    "for (ticker in Ticker_list){\n",
    "  Company_temp = {}\n",
    "  Monday_temp = {}\n",
    "  # Let's take one company at a time\n",
    "  Company_temp = filter(all_data_no_date,Ticker == ticker)\n",
    "  # We will use Monday as the benchmark for finding pairs\n",
    "  Monday_temp = filter(Company_temp,days == \"Monday\")\n",
    "  Monday = rbind(Monday, Monday_temp)\n",
    "  \n",
    "  for (date in Monday_temp$Date_num){\n",
    "    #Friday is 4 days after Monday\n",
    "    Friday_temp =  subset(Company_temp, Date_num == (date+4))\n",
    "    #If there is no data for the following Friday, set as NA\n",
    "    if(dim(Friday_temp)[1] == 0){Friday_temp = NA}\n",
    "    Friday = rbind(Friday,Friday_temp)\n",
    "    #We will repeat this for all other days of the week\n",
    "    Thursday_temp = subset(Company_temp, Date_num == (date+3))\n",
    "    if(dim(Thursday_temp)[1] == 0){Thursday_temp = NA}\n",
    "    Thursday = rbind(Thursday,Thursday_temp)\n",
    "    \n",
    "    Wednesday_temp = subset(Company_temp, Date_num == (date+2))\n",
    "    if(dim(Wednesday_temp)[1] == 0){Wednesday_temp = NA}\n",
    "    Wednesday = rbind(Wednesday,Wednesday_temp)\n",
    "    \n",
    "    Tuesday_temp = subset(Company_temp, Date_num == (date+1))\n",
    "    if(dim(Tuesday_temp)[1] == 0){Tuesday_temp = NA}\n",
    "    Tuesday = rbind(Tuesday,Tuesday_temp)\n",
    "    \n",
    "    Next_Monday_temp = subset(Company_temp, Date_num == (date+7))\n",
    "    if(dim(Next_Monday_temp)[1] == 0){Next_Monday_temp = NA}\n",
    "    Next_Monday = rbind(Next_Monday,Next_Monday_temp)\n",
    "  }\n",
    "}\n",
    "\n",
    "#Let's save the data sets for each days of the week to .csv files\n",
    "write.csv(Monday,\"Monday.csv\", row.names = FALSE)\n",
    "write.csv(Tuesday,\"Tuesday.csv\", row.names = FALSE)\n",
    "write.csv(Wednesday,\"Wednesday.csv\", row.names = FALSE)\n",
    "write.csv(Thursday,\"Thursday.csv\", row.names = FALSE)\n",
    "write.csv(Friday,\"Friday.csv\", row.names = FALSE)\n",
    "write.csv(Next_Monday,\"Next_Monday.csv\", row.names = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f1a400",
   "metadata": {},
   "source": [
    "### Reading in the datasets for each days of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ff4c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "#\n",
    "Monday = read_csv(\"Monday.csv\")\n",
    "Tuesday = read_csv(\"Tuesday.csv\")\n",
    "Wednesday = read_csv(\"Wednesday.csv\")\n",
    "Thursday = read_csv(\"Thursday.csv\")\n",
    "Friday = read_csv(\"Friday.csv\")\n",
    "Next_Monday = read_csv(\"Next_Monday.csv\")\n",
    "\n",
    "Monday['Index']= 1:nrow(Monday)\n",
    "Tuesday['Index']= 1:nrow(Tuesday)\n",
    "Wednesday['Index']= 1:nrow(Wednesday)\n",
    "Thursday['Index']= 1:nrow(Thursday)\n",
    "Friday['Index']= 1:nrow(Friday)\n",
    "Next_Monday['Index']= 1:nrow(Next_Monday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8db99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "#histogram of log-return on each day\n",
    "par(mfrow=c(1,5))\n",
    "hist(Monday$logreturn,xlim = c(-0.12,0.12),breaks = 40, main = \"Monday\", xlab = 'log returns')\n",
    "hist(Tuesday$logreturn,xlim = c(-0.12,0.12),breaks = 40, main = \"Tuesday\", xlab = 'log returns')\n",
    "hist(Wednesday$logreturn,xlim = c(-0.12,0.12),breaks = 40, main = \"Wednesday\", xlab = 'log returns')\n",
    "hist(Thursday$logreturn,xlim = c(-0.12,0.12),breaks = 40, main = \"Thursday\", xlab = 'log returns')\n",
    "hist(Friday$logreturn,xlim = c(-0.12,0.12),breaks = 40, main = \"Friday\", xlab = 'log returns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e46634",
   "metadata": {},
   "source": [
    "The histograms illustrate the frequency distribution of the log returns for each days of the week. Does this sound familiar? Yes, this is related to the first definition of Monday returns. By looking at the distribution, the log returns are close to zero for all days of the week. In fact, the number of days in which the log returns are slighly above zero is higher than thos slighly less than zero for all days of the week. Therefore, Monday effect in term of its first definition seems less promising. However, this section is related to the second definition of Monday effect. So, let's dive into examining the relationship between two consecutive days of the week. We will examine the linear regression on the pairs of consecutive days of the week.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8f5640",
   "metadata": {},
   "source": [
    "### Examining the relationship between the pairs of consecutive days of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d4cbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Let's look at the relationship between the log returns for pairs of consecutive days. A linear model is fit to the data.\n",
    "fit_F_M = lm(Friday$logreturn~Next_Monday$logreturn)\n",
    "fit_M_Tu = lm(Monday$logreturn~Tuesday$logreturn)\n",
    "fit_Tu_W = lm(Tuesday$logreturn~Wednesday$logreturn)\n",
    "fit_W_Th = lm(Wednesday$logreturn~Thursday$logreturn)\n",
    "fit_Th_F = lm(Thursday$logreturn~Friday$logreturn)\n",
    "tab_model(fit_F_M, fit_M_Tu, fit_Tu_W, fit_W_Th,fit_Th_F)\n",
    "# We will record the R-squared value of this linear regression. This will be used later on to compare it to other days of the week.\n",
    "fit_F_M_R2 = summary(fit_F_M)$r.squared\n",
    "fit_M_Tu_R2 = summary(fit_M_Tu)$r.squared\n",
    "fit_Tu_W_R2 = summary(fit_Tu_W)$r.squared\n",
    "fit_W_Th_R2 = summary(fit_W_Th)$r.squared\n",
    "fit_Th_F_R2 = summary(fit_Th_F)$r.squared\n",
    "# Let's create a series that has R-squared values of linear models using all industries on each pair of days\n",
    "r_before = c(fit_F_M_R2,fit_M_Tu_R2,fit_Tu_W_R2,fit_W_Th_R2,fit_Th_F_R2)\n",
    "\n",
    "# Let's visualize our data. We will draw a scatter plot and the linear regression. \n",
    "par(mfrow=c(5,1))\n",
    "plot(Friday$logreturn,Next_Monday$logreturn,\n",
    "    xlab = \"Friday\", ylab = \"Monday\")\n",
    "abline(fit_F_M)\n",
    "plot(Monday$logreturn,Tuesday$logreturn,\n",
    "    xlab = \"Monday\", ylab = \"Tuesday\")\n",
    "abline(fit_M_Tu)\n",
    "plot(Tuesday$logreturn,Wednesday$logreturn,\n",
    "    xlab = \"Tuesday\", ylab = \"Wednesday\")\n",
    "abline(fit_Tu_W)\n",
    "plot(Wednesday$logreturn,Thursday$logreturn,\n",
    "    xlab = \"Wednesday\", ylab = \"Thursday\")\n",
    "abline(fit_W_Th)\n",
    "plot(Thursday$logreturn,Friday$logreturn,\n",
    "    xlab = \"Thursday\", ylab = \"Friday\")\n",
    "abline(fit_Th_F)\n",
    "mtext(\"Linear relationship between two consecutive days\", outer = TRUE, cex = 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfe3b96",
   "metadata": {},
   "source": [
    "There is a statistically significant positive relationship between the log returns of Monday and Friday. So, Monday effect exisits? End of story? Not to soon. With this large of a sample size, a small relationship between the two values will be significant. The $R^2$ value indicates that around 0.8% of the variance in Monday returns are explained with the Friday returns. In other words, there is a statistically significant relationship, but the effect size is small. Additionally, we need to compare this to other days of the week. Perhaps, the small positive relationship is present in other days of the week. If that is the case, it is no longer a *Monday effect*; it is a consecutive day effect. So, let's get into examining the linear relationship between other consecutive days of the week.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d5897b",
   "metadata": {},
   "source": [
    "### Getting the pairs of consecutive days of the week"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
